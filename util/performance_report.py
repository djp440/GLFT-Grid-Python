#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ç›‘æ§æŠ¥å‘Šå·¥å…·
ç”¨äºç”Ÿæˆå’Œæ˜¾ç¤ºç³»ç»Ÿæ€§èƒ½æŠ¥å‘Š
"""

import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from util.performanceMonitor import get_performance_monitor
from util.sLogger import logger


class PerformanceReporter:
    """æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.monitor = get_performance_monitor()
    
    def generate_summary_report(self) -> Dict:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦æŠ¥å‘Š"""
        stats = self.monitor.get_stats()
        
        report = {
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "ç›‘æ§æ—¶é•¿": f"{(time.time() - self.monitor.start_time) / 3600:.2f}å°æ—¶",
            "æ€§èƒ½æŒ‡æ ‡": {}
        }
        
        for metric_type, data in stats.items():
            if data["count"] > 0:
                report["æ€§èƒ½æŒ‡æ ‡"][metric_type] = {
                    "æ€»æ¬¡æ•°": data["count"],
                    "æˆåŠŸæ¬¡æ•°": data["success_count"],
                    "å¤±è´¥æ¬¡æ•°": data["failure_count"],
                    "æˆåŠŸç‡": f"{data['success_rate']:.2f}%",
                    "å¹³å‡å»¶è¿Ÿ": f"{data['avg_latency']:.2f}ms",
                    "æœ€å°å»¶è¿Ÿ": f"{data['min_latency']:.2f}ms",
                    "æœ€å¤§å»¶è¿Ÿ": f"{data['max_latency']:.2f}ms",
                    "P95å»¶è¿Ÿ": f"{data['p95_latency']:.2f}ms",
                    "P99å»¶è¿Ÿ": f"{data['p99_latency']:.2f}ms"
                }
        
        return report
    
    def print_summary_report(self):
        """æ‰“å°æ€§èƒ½æ‘˜è¦æŠ¥å‘Š"""
        report = self.generate_summary_report()
        
        print("\n" + "="*60)
        print("ğŸ“Š ç³»ç»Ÿæ€§èƒ½ç›‘æ§æŠ¥å‘Š")
        print("="*60)
        print(f"ç”Ÿæˆæ—¶é—´: {report['ç”Ÿæˆæ—¶é—´']}")
        print(f"ç›‘æ§æ—¶é•¿: {report['ç›‘æ§æ—¶é•¿']}")
        print()
        
        if not report["æ€§èƒ½æŒ‡æ ‡"]:
            print("æš‚æ— æ€§èƒ½æ•°æ®")
            return
        
        for metric_type, data in report["æ€§èƒ½æŒ‡æ ‡"].items():
            print(f"ğŸ“ˆ {metric_type.upper()}æ€§èƒ½æŒ‡æ ‡:")
            print(f"  æ€»æ¬¡æ•°: {data['æ€»æ¬¡æ•°']}")
            print(f"  æˆåŠŸç‡: {data['æˆåŠŸç‡']}")
            print(f"  å¹³å‡å»¶è¿Ÿ: {data['å¹³å‡å»¶è¿Ÿ']}")
            print(f"  å»¶è¿ŸèŒƒå›´: {data['æœ€å°å»¶è¿Ÿ']} ~ {data['æœ€å¤§å»¶è¿Ÿ']}")
            print(f"  P95å»¶è¿Ÿ: {data['P95å»¶è¿Ÿ']}")
            print(f"  P99å»¶è¿Ÿ: {data['P99å»¶è¿Ÿ']}")
            
            # æ€§èƒ½è¯„çº§
            avg_latency = float(data['å¹³å‡å»¶è¿Ÿ'].replace('ms', ''))
            if avg_latency < 100:
                grade = "ğŸŸ¢ ä¼˜ç§€"
            elif avg_latency < 500:
                grade = "ğŸŸ¡ è‰¯å¥½"
            elif avg_latency < 1000:
                grade = "ğŸŸ  ä¸€èˆ¬"
            else:
                grade = "ğŸ”´ éœ€è¦ä¼˜åŒ–"
            
            print(f"  æ€§èƒ½è¯„çº§: {grade}")
            print()
    
    def save_report_to_file(self, filename: Optional[str] = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_report_{timestamp}.json"
        
        report = self.generate_summary_report()
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    def get_recent_latencies(self, metric_type: str, minutes: int = 10) -> List[float]:
        """è·å–æœ€è¿‘Nåˆ†é’Ÿçš„å»¶è¿Ÿæ•°æ®"""
        cutoff_time = time.time() - (minutes * 60)
        recent_latencies = []
        
        if metric_type in self.monitor.latencies:
            for latency, timestamp in self.monitor.latencies[metric_type]:
                if timestamp >= cutoff_time:
                    recent_latencies.append(latency)
        
        return recent_latencies
    
    def check_performance_alerts(self) -> List[str]:
        """æ£€æŸ¥æ€§èƒ½å‘Šè­¦"""
        alerts = []
        stats = self.monitor.get_stats()
        
        for metric_type, data in stats.items():
            if data["count"] > 0:
                # æ£€æŸ¥å¹³å‡å»¶è¿Ÿ
                if data["avg_latency"] > 1000:
                    alerts.append(f"âš ï¸ {metric_type}å¹³å‡å»¶è¿Ÿè¿‡é«˜: {data['avg_latency']:.2f}ms")
                
                # æ£€æŸ¥æˆåŠŸç‡
                if data["success_rate"] < 95:
                    alerts.append(f"âš ï¸ {metric_type}æˆåŠŸç‡è¿‡ä½: {data['success_rate']:.2f}%")
                
                # æ£€æŸ¥P99å»¶è¿Ÿ
                if data["p99_latency"] > 2000:
                    alerts.append(f"âš ï¸ {metric_type} P99å»¶è¿Ÿè¿‡é«˜: {data['p99_latency']:.2f}ms")
        
        return alerts


def main():
    """ä¸»å‡½æ•° - ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    reporter = PerformanceReporter()
    
    # æ‰“å°æ‘˜è¦æŠ¥å‘Š
    reporter.print_summary_report()
    
    # æ£€æŸ¥å‘Šè­¦
    alerts = reporter.check_performance_alerts()
    if alerts:
        print("ğŸš¨ æ€§èƒ½å‘Šè­¦:")
        for alert in alerts:
            print(f"  {alert}")
        print()
    
    # ä¿å­˜æŠ¥å‘Š
    reporter.save_report_to_file()


if __name__ == "__main__":
    main()